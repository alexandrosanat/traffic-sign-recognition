{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "TensorFlow version: 2.4.1\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print('TensorFlow version:',tf.__version__)\n",
    "print('Keras version:',tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.0\n",
      "125.0\n"
     ]
    }
   ],
   "source": [
    "classes = pd.read_csv(\"data/Train.csv\")\n",
    "\n",
    "min_width, max_width = max(classes.Width), min(classes.Width)\n",
    "min_height, max_height = max(classes.Height), min(classes.Height)\n",
    "\n",
    "print(np.mean([min_width, max_width]))\n",
    "print(np.mean([min_height, max_height]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43 unique classes in the dataset.\n"
     ]
    }
   ],
   "source": [
    "classes_no = len(classes.ClassId.unique())\n",
    "print(\"There are {} unique classes in the dataset.\".format(classes_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(input_shape=(125, 125, 3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # Add a dropout layer to avoid overfitting (drop 20%)\n",
    "# and a logistic layer\n",
    "predictions = Dense(classes_no, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "base_dir = os.path.join(cwd, 'data')\n",
    "train_path= os.path.join(base_dir, 'Train')\n",
    "test_path= os.path.join(base_dir, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "TARGET_SIZE = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31368 images belonging to 43 classes.\n",
      "Found 7841 images belonging to 43 classes.\n",
      "Found 12630 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a data generator for the training images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)  # val 20%\n",
    "\n",
    "# Create a data generator for the validation images\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "#Split data to training and validation datasets\n",
    "train_data = train_datagen.flow_from_directory(train_path, \n",
    "                                               target_size=TARGET_SIZE, \n",
    "                                               color_mode='rgb',\n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               seed=2,\n",
    "                                               subset = 'training') \n",
    "\n",
    "val_data = val_datagen.flow_from_directory(train_path, \n",
    "                                           target_size=TARGET_SIZE, \n",
    "                                           color_mode='rgb',\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=False,\n",
    "                                           seed=2,\n",
    "                                           subset = 'validation')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_data = datagen.flow_from_directory(test_path,\n",
    "                                        target_size=TARGET_SIZE, \n",
    "                                        color_mode='rgb',\n",
    "                                        class_mode='categorical',\n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='categorical_crossentropy', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_data,\n",
    "      steps_per_epoch= train_data.samples // BATCH_SIZE,  # One pass through entire training dataset\n",
    "      epochs=100,\n",
    "      validation_data=val_data,\n",
    "      validation_steps= val_data.samples // BATCH_SIZE,  # One pass through entire validation dataset\n",
    "      validation_freq=10,\n",
    "      callbacks = [callback],\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "\n",
    "history = model.fit(\n",
    "      train_data,\n",
    "      steps_per_epoch= train_data.samples // BATCH_SIZE,  # One pass through entire training dataset\n",
    "      epochs=50,\n",
    "      validation_data=val_data,\n",
    "      validation_steps= val_data.samples // BATCH_SIZE,  # One pass through entire validation dataset\n",
    "      #validation_freq=10,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model/my_model_50_epochs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(cwd, \"trained_model/my_model_50_epochs\")\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "test_image_path = glob.glob('Data/Test/*.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.expand_dims(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for img in test_image_path[0:1000]:   \n",
    "    img = tf.keras.preprocessing.image.load_img(img, target_size=(125, 125))\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    x = x/255\n",
    "    data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(test_data, steps=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_com=[]\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    img, label = test_data.next()\n",
    "    l = np.argmax(label[0])\n",
    "    print(l, predictions[i])\n",
    "    test_labels_com.append(l)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "number_of_examples = len(test_data.filenames)\n",
    "#number_of_generator_calls = math.ceil(number_of_examples / (1.0 * 1)) \n",
    "\n",
    "test_labels_com = []\n",
    "\n",
    "for i in range(0, int(number_of_examples)):\n",
    "    test_labels_com.append(np.argmax(test_data[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([np.argmax(predictions, axis=1), test_labels_com], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNEt 5 model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leNet():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "    model.add(layers.AveragePooling2D())\n",
    "\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.AveragePooling2D())\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(units=500, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(units=43, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "lenet = leNet()\n",
    "lenet.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1568/1568 [==============================] - 291s 186ms/step - loss: 2.3941 - accuracy: 0.2990 - val_loss: 2.3783 - val_accuracy: 0.2681\n",
      "Epoch 2/400\n",
      "1568/1568 [==============================] - 270s 172ms/step - loss: 2.3907 - accuracy: 0.3000 - val_loss: 2.3871 - val_accuracy: 0.2714\n",
      "Epoch 3/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3915 - accuracy: 0.3009 - val_loss: 2.3701 - val_accuracy: 0.2774\n",
      "Epoch 4/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3820 - accuracy: 0.3022 - val_loss: 2.3565 - val_accuracy: 0.2798\n",
      "Epoch 5/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3797 - accuracy: 0.3044 - val_loss: 2.3623 - val_accuracy: 0.2815\n",
      "Epoch 6/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3771 - accuracy: 0.3049 - val_loss: 2.3408 - val_accuracy: 0.2828\n",
      "Epoch 7/400\n",
      "1568/1568 [==============================] - 37s 24ms/step - loss: 2.3687 - accuracy: 0.3075 - val_loss: 2.3365 - val_accuracy: 0.2875\n",
      "Epoch 8/400\n",
      "1568/1568 [==============================] - 37s 24ms/step - loss: 2.3634 - accuracy: 0.3063 - val_loss: 2.3556 - val_accuracy: 0.2848TA: 2s - loss: 2.3655 - ac - ETA: 0s - loss: 2.3636 - accuracy: 0.30 - ETA: 0s - loss:\n",
      "Epoch 9/400\n",
      "1568/1568 [==============================] - 37s 24ms/step - loss: 2.3533 - accuracy: 0.3060 - val_loss: 2.3344 - val_accuracy: 0.2819\n",
      "Epoch 10/400\n",
      "1568/1568 [==============================] - 37s 23ms/step - loss: 2.3517 - accuracy: 0.3097 - val_loss: 2.3306 - val_accuracy: 0.2798- ETA: 19s - loss: 2.3568 - accuracy: 0 - ETA - ETA: 2s - loss: 2.3551 -  - E - ETA: 0s - loss: 2.3519 - accuracy: 0.30 - ETA: 0s - loss:\n",
      "Epoch 11/400\n",
      "1568/1568 [==============================] - 37s 24ms/step - loss: 2.3533 - accuracy: 0.3088 - val_loss: 2.3397 - val_accuracy: 0.2835\n",
      "Epoch 12/400\n",
      "1568/1568 [==============================] - 37s 23ms/step - loss: 2.3412 - accuracy: 0.3130 - val_loss: 2.3342 - val_accuracy: 0.2954\n",
      "Epoch 13/400\n",
      "1568/1568 [==============================] - 37s 23ms/step - loss: 2.3291 - accuracy: 0.3142 - val_loss: 2.3157 - val_accuracy: 0.3004\n",
      "Epoch 14/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3243 - accuracy: 0.3175 - val_loss: 2.3227 - val_accuracy: 0.28793296 - accuracy:  - ETA: 29s - loss: 2.335 - ETA: 28s - loss: 2.3275  - ETA: 27s - loss: 2.3406 -  - ETA: 26s - loss: 2. - ETA: 0s - loss: 2.3\n",
      "Epoch 15/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3236 - accuracy: 0.3143 - val_loss: 2.3217 - val_accuracy: 0.2880\n",
      "Epoch 16/400\n",
      "1568/1568 [==============================] - 39s 25ms/step - loss: 2.3171 - accuracy: 0.3193 - val_loss: 2.3258 - val_accuracy: 0.2904\n",
      "Epoch 17/400\n",
      "1568/1568 [==============================] - 39s 25ms/step - loss: 2.3108 - accuracy: 0.3210 - val_loss: 2.3042 - val_accuracy: 0.2957\n",
      "Epoch 18/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.3058 - accuracy: 0.3191 - val_loss: 2.3060 - val_accuracy: 0.2982\n",
      "Epoch 19/400\n",
      "1568/1568 [==============================] - 247s 158ms/step - loss: 2.3074 - accuracy: 0.3207 - val_loss: 2.3222 - val_accuracy: 0.2902\n",
      "Epoch 20/400\n",
      "1568/1568 [==============================] - 74s 47ms/step - loss: 2.3104 - accuracy: 0.3187 - val_loss: 2.3026 - val_accuracy: 0.2959\n",
      "Epoch 21/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2958 - accuracy: 0.3229 - val_loss: 2.3306 - val_accuracy: 0.2929\n",
      "Epoch 22/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2832 - accuracy: 0.3211 - val_loss: 2.2969 - val_accuracy: 0.2939\n",
      "Epoch 23/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2914 - accuracy: 0.3267 - val_loss: 2.2741 - val_accuracy: 0.2976loss: 2.3166 - accuracy: 0.319 - ETA: 31s - lo - ETA: 26s - loss: 2 - - ETA: 22s - loss: 2.2933 - acc - ETA: 21s - loss: 2.2987 - accurac - ETA: 21s - loss: 2.29 - ETA: 6s - loss: 2.2840 - accura - ETA: 4s - loss: 2\n",
      "Epoch 24/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2816 - accuracy: 0.3250 - val_loss: 2.2769 - val_accuracy: 0.312431s - loss: 2.2951 - accu - ETA: 30s - loss: 2.3030 - ETA: 1s - loss: 2.2825 - accura - ETA: 1s - loss: 2.2824 - accuracy: \n",
      "Epoch 25/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2682 - accuracy: 0.3288 - val_loss: 2.2695 - val_accuracy: 0.3061oss: 2.2734 - accu - ETA: 13s - loss: 2.2726 - accuracy: 0.327 - ETA: 13s - loss: 2.2723 - accur - ETA: 12 - ETA: 0s - loss: 2.2692 - ac - ETA: 0s - loss: 2.2691 - accura\n",
      "Epoch 26/400\n",
      "1568/1568 [==============================] - 39s 25ms/step - loss: 2.2660 - accuracy: 0.3288 - val_loss: 2.2815 - val_accuracy: 0.3102671 - ac - ETA: 3s - l\n",
      "Epoch 27/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2589 - accuracy: 0.3358 - val_loss: 2.2892 - val_accuracy: 0.2985TA: 30s - loss: 2.3092  - ETA: 29s - loss: 2.3025 - accurac - ETA: 29s - loss: 2.2871 - ac - ETA: 28s - loss: 2.2702 - accuracy:  - ETA: 28s - loss:  - ETA: 15s - loss: 2.2529 - accuracy:  - ETA: 15s - loss: 2.251\n",
      "Epoch 28/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2533 - accuracy: 0.3352 - val_loss: 2.2591 - val_accuracy: 0.3048.2566 - ac - - ETA: 4s - loss: 2.2568 - accuracy - ETA:  - ETA: 1s - loss: 2.2549 - accuracy: 0.\n",
      "Epoch 29/400\n",
      "1568/1568 [==============================] - 39s 25ms/step - loss: 2.2494 - accuracy: 0.3348 - val_loss: 2.2708 - val_accuracy: 0.3034\n",
      "Epoch 30/400\n",
      "1568/1568 [==============================] - 38s 24ms/step - loss: 2.2368 - accuracy: 0.3372 - val_loss: 2.2421 - val_accuracy: 0.3166\n",
      "Epoch 31/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 2.2448 - accuracy: 0.3362 - val_loss: 2.2699 - val_accuracy: 0.2996\n",
      "Epoch 32/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.2358 - accuracy: 0.3355 - val_loss: 2.2568 - val_accuracy: 0.3092\n",
      "Epoch 33/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.2331 - accuracy: 0.3408 - val_loss: 2.2292 - val_accuracy: 0.3163\n",
      "Epoch 34/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 2.2304 - accuracy: 0.3399 - val_loss: 2.2479 - val_accuracy: 0.3038\n",
      "Epoch 35/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 2.2227 - accuracy: 0.3430 - val_loss: 2.2110 - val_accuracy: 0.3242\n",
      "Epoch 36/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 2.2183 - accuracy: 0.3433 - val_loss: 2.2448 - val_accuracy: 0.3148\n",
      "Epoch 37/400\n",
      "1568/1568 [==============================] - 137s 87ms/step - loss: 2.2137 - accuracy: 0.3432 - val_loss: 2.2223 - val_accuracy: 0.3203\n",
      "Epoch 38/400\n",
      "1568/1568 [==============================] - 104s 67ms/step - loss: 2.2045 - accuracy: 0.3444 - val_loss: 2.2017 - val_accuracy: 0.3244\n",
      "Epoch 39/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 2.2001 - accuracy: 0.3470 - val_loss: 2.1917 - val_accuracy: 0.3292\n",
      "Epoch 40/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 2.2023 - accuracy: 0.3460 - val_loss: 2.1900 - val_accuracy: 0.3290\n",
      "Epoch 41/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 2.1894 - accuracy: 0.3485 - val_loss: 2.2264 - val_accuracy: 0.3082\n",
      "Epoch 42/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.1917 - accuracy: 0.3477 - val_loss: 2.1839 - val_accuracy: 0.3310\n",
      "Epoch 43/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.1991 - accuracy: 0.3483 - val_loss: 2.1922 - val_accuracy: 0.3256\n",
      "Epoch 44/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1797 - accuracy: 0.3522 - val_loss: 2.1805 - val_accuracy: 0.3300\n",
      "Epoch 45/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1776 - accuracy: 0.3503 - val_loss: 2.1884 - val_accuracy: 0.3309\n",
      "Epoch 46/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1642 - accuracy: 0.3539 - val_loss: 2.1736 - val_accuracy: 0.3325\n",
      "Epoch 47/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 2.1670 - accuracy: 0.3540 - val_loss: 2.1852 - val_accuracy: 0.3300\n",
      "Epoch 48/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.1642 - accuracy: 0.3565 - val_loss: 2.1880 - val_accuracy: 0.3256\n",
      "Epoch 49/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.1586 - accuracy: 0.3540 - val_loss: 2.1910 - val_accuracy: 0.3295\n",
      "Epoch 50/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1492 - accuracy: 0.3592 - val_loss: 2.1641 - val_accuracy: 0.3330\n",
      "Epoch 51/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.1464 - accuracy: 0.3538 - val_loss: 2.1701 - val_accuracy: 0.3353\n",
      "Epoch 52/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.1391 - accuracy: 0.3603 - val_loss: 2.1563 - val_accuracy: 0.3444\n",
      "Epoch 53/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.1349 - accuracy: 0.3631 - val_loss: 2.1563 - val_accuracy: 0.3353\n",
      "Epoch 54/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.1369 - accuracy: 0.3654 - val_loss: 2.1626 - val_accuracy: 0.3293\n",
      "Epoch 55/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.1279 - accuracy: 0.3619 - val_loss: 2.1838 - val_accuracy: 0.3258\n",
      "Epoch 56/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.1240 - accuracy: 0.3640 - val_loss: 2.1375 - val_accuracy: 0.3438\n",
      "Epoch 57/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.1200 - accuracy: 0.3666 - val_loss: 2.1360 - val_accuracy: 0.3457\n",
      "Epoch 58/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.1201 - accuracy: 0.3692 - val_loss: 2.1299 - val_accuracy: 0.3407\n",
      "Epoch 59/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1246 - accuracy: 0.3649 - val_loss: 2.1334 - val_accuracy: 0.3389\n",
      "Epoch 60/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1088 - accuracy: 0.3689 - val_loss: 2.1535 - val_accuracy: 0.3401\n",
      "Epoch 61/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.1064 - accuracy: 0.3694 - val_loss: 2.1274 - val_accuracy: 0.3490\n",
      "Epoch 62/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 2.0941 - accuracy: 0.3724 - val_loss: 2.1275 - val_accuracy: 0.3443\n",
      "Epoch 63/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0935 - accuracy: 0.3741 - val_loss: 2.0951 - val_accuracy: 0.3487\n",
      "Epoch 64/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0960 - accuracy: 0.3738 - val_loss: 2.1090 - val_accuracy: 0.3462\n",
      "Epoch 65/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0914 - accuracy: 0.3725 - val_loss: 2.1283 - val_accuracy: 0.3397\n",
      "Epoch 66/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0843 - accuracy: 0.3767 - val_loss: 2.1409 - val_accuracy: 0.3439\n",
      "Epoch 67/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0832 - accuracy: 0.3751 - val_loss: 2.1302 - val_accuracy: 0.3413\n",
      "Epoch 68/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0753 - accuracy: 0.3757 - val_loss: 2.0894 - val_accuracy: 0.3518\n",
      "Epoch 69/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0736 - accuracy: 0.3754 - val_loss: 2.1061 - val_accuracy: 0.3496\n",
      "Epoch 70/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 2.0734 - accuracy: 0.3782 - val_loss: 2.1099 - val_accuracy: 0.3404\n",
      "Epoch 71/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0595 - accuracy: 0.3830 - val_loss: 2.0900 - val_accuracy: 0.3620\n",
      "Epoch 72/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0602 - accuracy: 0.3790 - val_loss: 2.1023 - val_accuracy: 0.3504\n",
      "Epoch 73/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.0622 - accuracy: 0.3808 - val_loss: 2.0735 - val_accuracy: 0.3579\n",
      "Epoch 74/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0558 - accuracy: 0.3827 - val_loss: 2.0918 - val_accuracy: 0.3582\n",
      "Epoch 75/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.0578 - accuracy: 0.3817 - val_loss: 2.0863 - val_accuracy: 0.3602\n",
      "Epoch 76/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0422 - accuracy: 0.3853 - val_loss: 2.0625 - val_accuracy: 0.3611\n",
      "Epoch 77/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0427 - accuracy: 0.3862 - val_loss: 2.0477 - val_accuracy: 0.3625\n",
      "Epoch 78/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.0302 - accuracy: 0.3871 - val_loss: 2.0948 - val_accuracy: 0.3531\n",
      "Epoch 79/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0391 - accuracy: 0.3898 - val_loss: 2.1121 - val_accuracy: 0.3448\n",
      "Epoch 80/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.0320 - accuracy: 0.3914 - val_loss: 2.0545 - val_accuracy: 0.3631\n",
      "Epoch 81/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0240 - accuracy: 0.3886 - val_loss: 2.0502 - val_accuracy: 0.3713\n",
      "Epoch 82/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0291 - accuracy: 0.3911 - val_loss: 2.0925 - val_accuracy: 0.3520\n",
      "Epoch 83/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.0147 - accuracy: 0.3898 - val_loss: 2.0764 - val_accuracy: 0.3559\n",
      "Epoch 84/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 2.0200 - accuracy: 0.3897 - val_loss: 2.0444 - val_accuracy: 0.3608\n",
      "Epoch 85/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.0118 - accuracy: 0.3941 - val_loss: 2.0494 - val_accuracy: 0.3724\n",
      "Epoch 86/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 2.0090 - accuracy: 0.3965 - val_loss: 2.0397 - val_accuracy: 0.3718\n",
      "Epoch 87/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 2.0134 - accuracy: 0.3929 - val_loss: 2.0536 - val_accuracy: 0.3666\n",
      "Epoch 88/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 2.0012 - accuracy: 0.3929 - val_loss: 2.0439 - val_accuracy: 0.3652\n",
      "Epoch 89/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9974 - accuracy: 0.3975 - val_loss: 2.0726 - val_accuracy: 0.3552\n",
      "Epoch 90/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.9887 - accuracy: 0.3975 - val_loss: 2.0440 - val_accuracy: 0.3702\n",
      "Epoch 91/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.9866 - accuracy: 0.3978 - val_loss: 2.0212 - val_accuracy: 0.3763\n",
      "Epoch 92/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.9808 - accuracy: 0.4002 - val_loss: 2.0181 - val_accuracy: 0.3848\n",
      "Epoch 93/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.9862 - accuracy: 0.4021 - val_loss: 2.0261 - val_accuracy: 0.3723\n",
      "Epoch 94/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9797 - accuracy: 0.3989 - val_loss: 2.0250 - val_accuracy: 0.3737\n",
      "Epoch 95/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9766 - accuracy: 0.4042 - val_loss: 2.0115 - val_accuracy: 0.3932\n",
      "Epoch 96/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.9684 - accuracy: 0.4067 - val_loss: 2.0098 - val_accuracy: 0.3911\n",
      "Epoch 97/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.9744 - accuracy: 0.4049 - val_loss: 2.0106 - val_accuracy: 0.3760\n",
      "Epoch 98/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.9655 - accuracy: 0.4042 - val_loss: 2.0235 - val_accuracy: 0.3857\n",
      "Epoch 99/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9575 - accuracy: 0.4086 - val_loss: 2.0045 - val_accuracy: 0.3883\n",
      "Epoch 100/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9651 - accuracy: 0.4051 - val_loss: 1.9993 - val_accuracy: 0.3890\n",
      "Epoch 101/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9541 - accuracy: 0.4090 - val_loss: 1.9818 - val_accuracy: 0.3897\n",
      "Epoch 102/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9462 - accuracy: 0.4098 - val_loss: 1.9924 - val_accuracy: 0.3902\n",
      "Epoch 103/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9423 - accuracy: 0.4099 - val_loss: 1.9840 - val_accuracy: 0.3886\n",
      "Epoch 104/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9515 - accuracy: 0.4066 - val_loss: 1.9942 - val_accuracy: 0.3858\n",
      "Epoch 105/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9391 - accuracy: 0.4116 - val_loss: 1.9898 - val_accuracy: 0.3807\n",
      "Epoch 106/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9380 - accuracy: 0.4122 - val_loss: 1.9663 - val_accuracy: 0.4041\n",
      "Epoch 107/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9429 - accuracy: 0.4116 - val_loss: 1.9675 - val_accuracy: 0.3936\n",
      "Epoch 108/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9316 - accuracy: 0.4170 - val_loss: 1.9839 - val_accuracy: 0.3837\n",
      "Epoch 109/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9339 - accuracy: 0.4123 - val_loss: 1.9960 - val_accuracy: 0.3788\n",
      "Epoch 110/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.9268 - accuracy: 0.4158 - val_loss: 1.9819 - val_accuracy: 0.3820\n",
      "Epoch 111/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 1.9307 - accuracy: 0.4145 - val_loss: 1.9631 - val_accuracy: 0.3931\n",
      "Epoch 112/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 1.9190 - accuracy: 0.4190 - val_loss: 1.9898 - val_accuracy: 0.3776\n",
      "Epoch 113/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 1.9140 - accuracy: 0.4159 - val_loss: 1.9929 - val_accuracy: 0.3906\n",
      "Epoch 114/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 1.9077 - accuracy: 0.4177 - val_loss: 1.9835 - val_accuracy: 0.3922\n",
      "Epoch 115/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 1.9136 - accuracy: 0.4184 - val_loss: 1.9747 - val_accuracy: 0.3952\n",
      "Epoch 116/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.9047 - accuracy: 0.4215 - val_loss: 1.9544 - val_accuracy: 0.3939\n",
      "Epoch 117/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8975 - accuracy: 0.4217 - val_loss: 1.9455 - val_accuracy: 0.3892\n",
      "Epoch 118/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.9039 - accuracy: 0.4222 - val_loss: 1.9349 - val_accuracy: 0.3992\n",
      "Epoch 119/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.9000 - accuracy: 0.4214 - val_loss: 1.9480 - val_accuracy: 0.3943\n",
      "Epoch 120/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8944 - accuracy: 0.4249 - val_loss: 1.9441 - val_accuracy: 0.3974\n",
      "Epoch 121/400\n",
      "1568/1568 [==============================] - 31s 20ms/step - loss: 1.8931 - accuracy: 0.4266 - val_loss: 1.9581 - val_accuracy: 0.4087\n",
      "Epoch 122/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.8855 - accuracy: 0.4277 - val_loss: 1.9421 - val_accuracy: 0.3974\n",
      "Epoch 123/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.8771 - accuracy: 0.4274 - val_loss: 1.9052 - val_accuracy: 0.4093\n",
      "Epoch 124/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.8737 - accuracy: 0.4293 - val_loss: 1.9341 - val_accuracy: 0.4055\n",
      "Epoch 125/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.8795 - accuracy: 0.4277 - val_loss: 1.9276 - val_accuracy: 0.4077\n",
      "Epoch 126/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.8847 - accuracy: 0.4231 - val_loss: 1.9495 - val_accuracy: 0.3958\n",
      "Epoch 127/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8760 - accuracy: 0.4300 - val_loss: 1.9037 - val_accuracy: 0.4176\n",
      "Epoch 128/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.8799 - accuracy: 0.4289 - val_loss: 1.9232 - val_accuracy: 0.4138\n",
      "Epoch 129/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.8625 - accuracy: 0.4332 - val_loss: 1.9164 - val_accuracy: 0.4144\n",
      "Epoch 130/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8616 - accuracy: 0.4293 - val_loss: 1.9018 - val_accuracy: 0.4082\n",
      "Epoch 131/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.8650 - accuracy: 0.4295 - val_loss: 1.9462 - val_accuracy: 0.4066\n",
      "Epoch 132/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.8554 - accuracy: 0.4349 - val_loss: 1.8978 - val_accuracy: 0.4165\n",
      "Epoch 133/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.8539 - accuracy: 0.4330 - val_loss: 1.8996 - val_accuracy: 0.4190\n",
      "Epoch 134/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.8551 - accuracy: 0.4327 - val_loss: 1.8887 - val_accuracy: 0.4184\n",
      "Epoch 135/400\n",
      "1568/1568 [==============================] - 35s 23ms/step - loss: 1.8393 - accuracy: 0.4395 - val_loss: 1.9538 - val_accuracy: 0.3945\n",
      "Epoch 136/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 1.8379 - accuracy: 0.4412 - val_loss: 1.9006 - val_accuracy: 0.4177\n",
      "Epoch 137/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.8318 - accuracy: 0.4373 - val_loss: 1.9087 - val_accuracy: 0.4131\n",
      "Epoch 138/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.8401 - accuracy: 0.4368 - val_loss: 1.9192 - val_accuracy: 0.4130\n",
      "Epoch 139/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 1.8446 - accuracy: 0.4332 - val_loss: 1.8956 - val_accuracy: 0.4291\n",
      "Epoch 140/400\n",
      "1568/1568 [==============================] - 95s 61ms/step - loss: 1.8266 - accuracy: 0.4409 - val_loss: 1.9245 - val_accuracy: 0.4085\n",
      "Epoch 141/400\n",
      "1568/1568 [==============================] - 165s 105ms/step - loss: 1.8313 - accuracy: 0.4359 - val_loss: 1.8605 - val_accuracy: 0.4302\n",
      "Epoch 142/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8283 - accuracy: 0.4409 - val_loss: 1.8954 - val_accuracy: 0.4099\n",
      "Epoch 143/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8196 - accuracy: 0.4437 - val_loss: 1.8995 - val_accuracy: 0.4142\n",
      "Epoch 144/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.8185 - accuracy: 0.4436 - val_loss: 1.8857 - val_accuracy: 0.4129\n",
      "Epoch 145/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8195 - accuracy: 0.4418 - val_loss: 1.8673 - val_accuracy: 0.4309\n",
      "Epoch 146/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8100 - accuracy: 0.4446 - val_loss: 1.8882 - val_accuracy: 0.4159\n",
      "Epoch 147/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.8104 - accuracy: 0.4424 - val_loss: 1.8451 - val_accuracy: 0.4365\n",
      "Epoch 148/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.8115 - accuracy: 0.4472 - val_loss: 1.8850 - val_accuracy: 0.4256\n",
      "Epoch 149/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7948 - accuracy: 0.4470 - val_loss: 1.8608 - val_accuracy: 0.4263\n",
      "Epoch 150/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7936 - accuracy: 0.4477 - val_loss: 1.8692 - val_accuracy: 0.4239\n",
      "Epoch 151/400\n",
      "1568/1568 [==============================] - 37s 24ms/step - loss: 1.8025 - accuracy: 0.4512 - val_loss: 1.8508 - val_accuracy: 0.4392\n",
      "Epoch 152/400\n",
      "1568/1568 [==============================] - 35s 23ms/step - loss: 1.7984 - accuracy: 0.4490 - val_loss: 1.8665 - val_accuracy: 0.4362\n",
      "Epoch 153/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7956 - accuracy: 0.4505 - val_loss: 1.8610 - val_accuracy: 0.4223\n",
      "Epoch 154/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7888 - accuracy: 0.4504 - val_loss: 1.8803 - val_accuracy: 0.4255\n",
      "Epoch 155/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7967 - accuracy: 0.4504 - val_loss: 1.8551 - val_accuracy: 0.4364\n",
      "Epoch 156/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7845 - accuracy: 0.4500 - val_loss: 1.8500 - val_accuracy: 0.4402\n",
      "Epoch 157/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.7736 - accuracy: 0.4551 - val_loss: 1.8620 - val_accuracy: 0.4291\n",
      "Epoch 158/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.7800 - accuracy: 0.4551 - val_loss: 1.8683 - val_accuracy: 0.4255\n",
      "Epoch 159/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7738 - accuracy: 0.4515 - val_loss: 1.8356 - val_accuracy: 0.4395\n",
      "Epoch 160/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.7764 - accuracy: 0.4557 - val_loss: 1.8418 - val_accuracy: 0.4329\n",
      "Epoch 161/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7709 - accuracy: 0.4563 - val_loss: 1.8610 - val_accuracy: 0.4332\n",
      "Epoch 162/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7705 - accuracy: 0.4542 - val_loss: 1.8274 - val_accuracy: 0.4390\n",
      "Epoch 163/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7659 - accuracy: 0.4590 - val_loss: 1.8643 - val_accuracy: 0.4374\n",
      "Epoch 164/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7633 - accuracy: 0.4596 - val_loss: 1.8640 - val_accuracy: 0.4304\n",
      "Epoch 165/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7641 - accuracy: 0.4570 - val_loss: 1.8534 - val_accuracy: 0.4406\n",
      "Epoch 166/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7650 - accuracy: 0.4554 - val_loss: 1.8696 - val_accuracy: 0.4314\n",
      "Epoch 167/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7537 - accuracy: 0.4569 - val_loss: 1.8231 - val_accuracy: 0.4473\n",
      "Epoch 168/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7537 - accuracy: 0.4582 - val_loss: 1.8575 - val_accuracy: 0.4388\n",
      "Epoch 169/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7513 - accuracy: 0.4606 - val_loss: 1.8360 - val_accuracy: 0.4374\n",
      "Epoch 170/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7421 - accuracy: 0.4625 - val_loss: 1.8388 - val_accuracy: 0.4374\n",
      "Epoch 171/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7513 - accuracy: 0.4624 - val_loss: 1.8044 - val_accuracy: 0.4466\n",
      "Epoch 172/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7496 - accuracy: 0.4626 - val_loss: 1.8348 - val_accuracy: 0.4338\n",
      "Epoch 173/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7433 - accuracy: 0.4616 - val_loss: 1.8511 - val_accuracy: 0.4357\n",
      "Epoch 174/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7322 - accuracy: 0.4694 - val_loss: 1.8316 - val_accuracy: 0.4494\n",
      "Epoch 175/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7377 - accuracy: 0.4662 - val_loss: 1.8535 - val_accuracy: 0.4235\n",
      "Epoch 176/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7358 - accuracy: 0.4648 - val_loss: 1.8301 - val_accuracy: 0.4398\n",
      "Epoch 177/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7344 - accuracy: 0.4626 - val_loss: 1.8233 - val_accuracy: 0.4459\n",
      "Epoch 178/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7333 - accuracy: 0.4641 - val_loss: 1.8011 - val_accuracy: 0.4533\n",
      "Epoch 179/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7380 - accuracy: 0.4644 - val_loss: 1.8068 - val_accuracy: 0.4490\n",
      "Epoch 180/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7289 - accuracy: 0.4640 - val_loss: 1.7897 - val_accuracy: 0.4497\n",
      "Epoch 181/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7364 - accuracy: 0.4644 - val_loss: 1.8180 - val_accuracy: 0.4486\n",
      "Epoch 182/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7183 - accuracy: 0.4696 - val_loss: 1.8123 - val_accuracy: 0.4490\n",
      "Epoch 183/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7296 - accuracy: 0.4648 - val_loss: 1.7956 - val_accuracy: 0.4560\n",
      "Epoch 184/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7146 - accuracy: 0.4708 - val_loss: 1.8170 - val_accuracy: 0.4444\n",
      "Epoch 185/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.7201 - accuracy: 0.4666 - val_loss: 1.7952 - val_accuracy: 0.4492\n",
      "Epoch 186/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 1.7104 - accuracy: 0.4681 - val_loss: 1.8265 - val_accuracy: 0.4494\n",
      "Epoch 187/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7062 - accuracy: 0.4707 - val_loss: 1.7975 - val_accuracy: 0.4557\n",
      "Epoch 188/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.7054 - accuracy: 0.4727 - val_loss: 1.7888 - val_accuracy: 0.4554\n",
      "Epoch 189/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.6954 - accuracy: 0.4747 - val_loss: 1.8183 - val_accuracy: 0.4460\n",
      "Epoch 190/400\n",
      "1568/1568 [==============================] - 220s 141ms/step - loss: 1.7067 - accuracy: 0.4692 - val_loss: 1.8021 - val_accuracy: 0.4448\n",
      "Epoch 191/400\n",
      "1568/1568 [==============================] - 42s 27ms/step - loss: 1.7138 - accuracy: 0.4755 - val_loss: 1.8031 - val_accuracy: 0.4453\n",
      "Epoch 192/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6894 - accuracy: 0.4784 - val_loss: 1.7962 - val_accuracy: 0.4529\n",
      "Epoch 193/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.7014 - accuracy: 0.4751 - val_loss: 1.8118 - val_accuracy: 0.4526\n",
      "Epoch 194/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.7001 - accuracy: 0.4724 - val_loss: 1.7929 - val_accuracy: 0.4486\n",
      "Epoch 195/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6855 - accuracy: 0.4764 - val_loss: 1.8125 - val_accuracy: 0.4536\n",
      "Epoch 196/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6938 - accuracy: 0.4754 - val_loss: 1.7857 - val_accuracy: 0.4466\n",
      "Epoch 197/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6965 - accuracy: 0.4776 - val_loss: 1.8016 - val_accuracy: 0.4508\n",
      "Epoch 198/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6858 - accuracy: 0.4765 - val_loss: 1.7862 - val_accuracy: 0.4527\n",
      "Epoch 199/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6879 - accuracy: 0.4767 - val_loss: 1.7728 - val_accuracy: 0.4636\n",
      "Epoch 200/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6764 - accuracy: 0.4777 - val_loss: 1.7928 - val_accuracy: 0.4573\n",
      "Epoch 201/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6732 - accuracy: 0.4811 - val_loss: 1.8147 - val_accuracy: 0.4486\n",
      "Epoch 202/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6810 - accuracy: 0.4797 - val_loss: 1.8070 - val_accuracy: 0.4520\n",
      "Epoch 203/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6727 - accuracy: 0.4802 - val_loss: 1.7916 - val_accuracy: 0.4593\n",
      "Epoch 204/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6732 - accuracy: 0.4815 - val_loss: 1.7903 - val_accuracy: 0.4607\n",
      "Epoch 205/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6701 - accuracy: 0.4773 - val_loss: 1.7887 - val_accuracy: 0.4476\n",
      "Epoch 206/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6698 - accuracy: 0.4827 - val_loss: 1.7864 - val_accuracy: 0.4430\n",
      "Epoch 207/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6765 - accuracy: 0.4810 - val_loss: 1.7763 - val_accuracy: 0.4563\n",
      "Epoch 208/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6635 - accuracy: 0.4853 - val_loss: 1.7683 - val_accuracy: 0.4543\n",
      "Epoch 209/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6690 - accuracy: 0.4803 - val_loss: 1.7782 - val_accuracy: 0.4597\n",
      "Epoch 210/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6647 - accuracy: 0.4841 - val_loss: 1.7878 - val_accuracy: 0.4588\n",
      "Epoch 211/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6529 - accuracy: 0.4870 - val_loss: 1.7854 - val_accuracy: 0.4569\n",
      "Epoch 212/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6582 - accuracy: 0.4862 - val_loss: 1.7874 - val_accuracy: 0.4477\n",
      "Epoch 213/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6553 - accuracy: 0.4857 - val_loss: 1.7780 - val_accuracy: 0.4505\n",
      "Epoch 214/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6462 - accuracy: 0.4873 - val_loss: 1.7866 - val_accuracy: 0.4542\n",
      "Epoch 215/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6516 - accuracy: 0.4873 - val_loss: 1.7842 - val_accuracy: 0.4431\n",
      "Epoch 216/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6461 - accuracy: 0.4890 - val_loss: 1.7927 - val_accuracy: 0.4480\n",
      "Epoch 217/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6436 - accuracy: 0.4893 - val_loss: 1.7577 - val_accuracy: 0.4527\n",
      "Epoch 218/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6464 - accuracy: 0.4857 - val_loss: 1.7878 - val_accuracy: 0.4552\n",
      "Epoch 219/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6347 - accuracy: 0.4885 - val_loss: 1.7931 - val_accuracy: 0.4517\n",
      "Epoch 220/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6330 - accuracy: 0.4915 - val_loss: 1.7789 - val_accuracy: 0.4628\n",
      "Epoch 221/400\n",
      "1568/1568 [==============================] - 32s 20ms/step - loss: 1.6319 - accuracy: 0.4919 - val_loss: 1.7690 - val_accuracy: 0.4524\n",
      "Epoch 222/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6395 - accuracy: 0.4905 - val_loss: 1.7752 - val_accuracy: 0.4529\n",
      "Epoch 223/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.6370 - accuracy: 0.4891 - val_loss: 1.7691 - val_accuracy: 0.4597\n",
      "Epoch 224/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.6359 - accuracy: 0.4933 - val_loss: 1.7706 - val_accuracy: 0.4601\n",
      "Epoch 225/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6340 - accuracy: 0.4897 - val_loss: 1.7796 - val_accuracy: 0.4441\n",
      "Epoch 226/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6328 - accuracy: 0.4936 - val_loss: 1.7498 - val_accuracy: 0.4636\n",
      "Epoch 227/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6172 - accuracy: 0.4941 - val_loss: 1.7413 - val_accuracy: 0.4667\n",
      "Epoch 228/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.6224 - accuracy: 0.4974 - val_loss: 1.7645 - val_accuracy: 0.4645\n",
      "Epoch 229/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6172 - accuracy: 0.4959 - val_loss: 1.7739 - val_accuracy: 0.4496\n",
      "Epoch 230/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.6350 - accuracy: 0.4892 - val_loss: 1.7669 - val_accuracy: 0.4602\n",
      "Epoch 231/400\n",
      "1568/1568 [==============================] - 37s 24ms/step - loss: 1.6190 - accuracy: 0.4949 - val_loss: 1.7560 - val_accuracy: 0.4642\n",
      "Epoch 232/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.6184 - accuracy: 0.4958 - val_loss: 1.7613 - val_accuracy: 0.4570\n",
      "Epoch 233/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6165 - accuracy: 0.4977 - val_loss: 1.7680 - val_accuracy: 0.4563\n",
      "Epoch 234/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6091 - accuracy: 0.4983 - val_loss: 1.7684 - val_accuracy: 0.4519\n",
      "Epoch 235/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.6053 - accuracy: 0.4977 - val_loss: 1.7605 - val_accuracy: 0.4658\n",
      "Epoch 236/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.6115 - accuracy: 0.4967 - val_loss: 1.7616 - val_accuracy: 0.4605\n",
      "Epoch 237/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.6090 - accuracy: 0.4983 - val_loss: 1.7378 - val_accuracy: 0.4662\n",
      "Epoch 238/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5988 - accuracy: 0.5035 - val_loss: 1.7507 - val_accuracy: 0.4551\n",
      "Epoch 239/400\n",
      "1568/1568 [==============================] - 193s 123ms/step - loss: 1.6045 - accuracy: 0.5008 - val_loss: 1.7519 - val_accuracy: 0.4644\n",
      "Epoch 240/400\n",
      "1568/1568 [==============================] - 76s 48ms/step - loss: 1.6055 - accuracy: 0.5003 - val_loss: 1.7641 - val_accuracy: 0.4694\n",
      "Epoch 241/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.6046 - accuracy: 0.5001 - val_loss: 1.7465 - val_accuracy: 0.4709\n",
      "Epoch 242/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.6061 - accuracy: 0.5009 - val_loss: 1.7550 - val_accuracy: 0.4611\n",
      "Epoch 243/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.6000 - accuracy: 0.5000 - val_loss: 1.7710 - val_accuracy: 0.4497\n",
      "Epoch 244/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.6009 - accuracy: 0.4987 - val_loss: 1.7732 - val_accuracy: 0.4593\n",
      "Epoch 245/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5907 - accuracy: 0.5026 - val_loss: 1.7199 - val_accuracy: 0.4630\n",
      "Epoch 246/400\n",
      "1568/1568 [==============================] - 36s 23ms/step - loss: 1.5986 - accuracy: 0.5040 - val_loss: 1.7890 - val_accuracy: 0.4499\n",
      "Epoch 247/400\n",
      "1568/1568 [==============================] - 35s 22ms/step - loss: 1.5930 - accuracy: 0.5038 - val_loss: 1.7369 - val_accuracy: 0.4642\n",
      "Epoch 248/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5955 - accuracy: 0.5028 - val_loss: 1.7220 - val_accuracy: 0.4671\n",
      "Epoch 249/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5915 - accuracy: 0.5016 - val_loss: 1.7562 - val_accuracy: 0.4628\n",
      "Epoch 250/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5872 - accuracy: 0.5013 - val_loss: 1.7535 - val_accuracy: 0.4617\n",
      "Epoch 251/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5840 - accuracy: 0.4983 - val_loss: 1.7417 - val_accuracy: 0.4617\n",
      "Epoch 252/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.5837 - accuracy: 0.5041 - val_loss: 1.7301 - val_accuracy: 0.4643\n",
      "Epoch 253/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5771 - accuracy: 0.5040 - val_loss: 1.7330 - val_accuracy: 0.4691\n",
      "Epoch 254/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.5875 - accuracy: 0.5033 - val_loss: 1.7256 - val_accuracy: 0.4596\n",
      "Epoch 255/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5742 - accuracy: 0.5061 - val_loss: 1.7227 - val_accuracy: 0.4686\n",
      "Epoch 256/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5702 - accuracy: 0.5061 - val_loss: 1.7277 - val_accuracy: 0.4653\n",
      "Epoch 257/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5735 - accuracy: 0.5076 - val_loss: 1.7232 - val_accuracy: 0.4628\n",
      "Epoch 258/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5650 - accuracy: 0.5134 - val_loss: 1.7253 - val_accuracy: 0.4724\n",
      "Epoch 259/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5643 - accuracy: 0.5128 - val_loss: 1.7206 - val_accuracy: 0.4698\n",
      "Epoch 260/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5671 - accuracy: 0.5112 - val_loss: 1.7155 - val_accuracy: 0.4736\n",
      "Epoch 261/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5669 - accuracy: 0.5058 - val_loss: 1.7350 - val_accuracy: 0.4746\n",
      "Epoch 262/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5603 - accuracy: 0.5078 - val_loss: 1.7067 - val_accuracy: 0.4842\n",
      "Epoch 263/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.5583 - accuracy: 0.5099 - val_loss: 1.7364 - val_accuracy: 0.4639\n",
      "Epoch 264/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5575 - accuracy: 0.5115 - val_loss: 1.7081 - val_accuracy: 0.4733\n",
      "Epoch 265/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5520 - accuracy: 0.5138 - val_loss: 1.7220 - val_accuracy: 0.4746\n",
      "Epoch 266/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5547 - accuracy: 0.5130 - val_loss: 1.7324 - val_accuracy: 0.4746\n",
      "Epoch 267/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5609 - accuracy: 0.5066 - val_loss: 1.7227 - val_accuracy: 0.4584\n",
      "Epoch 268/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5728 - accuracy: 0.5098 - val_loss: 1.7356 - val_accuracy: 0.4577\n",
      "Epoch 269/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5504 - accuracy: 0.5140 - val_loss: 1.7249 - val_accuracy: 0.4640\n",
      "Epoch 270/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5603 - accuracy: 0.5115 - val_loss: 1.7316 - val_accuracy: 0.4654\n",
      "Epoch 271/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5525 - accuracy: 0.5135 - val_loss: 1.6999 - val_accuracy: 0.4823\n",
      "Epoch 272/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5512 - accuracy: 0.5112 - val_loss: 1.7063 - val_accuracy: 0.4709\n",
      "Epoch 273/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5525 - accuracy: 0.5107 - val_loss: 1.7272 - val_accuracy: 0.4656\n",
      "Epoch 274/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5501 - accuracy: 0.5142 - val_loss: 1.7079 - val_accuracy: 0.4565\n",
      "Epoch 275/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5391 - accuracy: 0.5155 - val_loss: 1.7110 - val_accuracy: 0.4784\n",
      "Epoch 276/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5416 - accuracy: 0.5142 - val_loss: 1.7073 - val_accuracy: 0.4735\n",
      "Epoch 277/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5427 - accuracy: 0.5163 - val_loss: 1.7452 - val_accuracy: 0.4700\n",
      "Epoch 278/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5424 - accuracy: 0.5106 - val_loss: 1.7133 - val_accuracy: 0.4657\n",
      "Epoch 279/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5374 - accuracy: 0.5170 - val_loss: 1.7099 - val_accuracy: 0.4783\n",
      "Epoch 280/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.5314 - accuracy: 0.5204 - val_loss: 1.7134 - val_accuracy: 0.4736\n",
      "Epoch 281/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5474 - accuracy: 0.5106 - val_loss: 1.6941 - val_accuracy: 0.4761\n",
      "Epoch 282/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5309 - accuracy: 0.5190 - val_loss: 1.7063 - val_accuracy: 0.4781\n",
      "Epoch 283/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5233 - accuracy: 0.5184 - val_loss: 1.7360 - val_accuracy: 0.4689\n",
      "Epoch 284/400\n",
      "1568/1568 [==============================] - 34s 21ms/step - loss: 1.5384 - accuracy: 0.5180 - val_loss: 1.7211 - val_accuracy: 0.4730\n",
      "Epoch 285/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5169 - accuracy: 0.5194 - val_loss: 1.6731 - val_accuracy: 0.4841\n",
      "Epoch 286/400\n",
      "1568/1568 [==============================] - 34s 22ms/step - loss: 1.5331 - accuracy: 0.5177 - val_loss: 1.7239 - val_accuracy: 0.4777\n",
      "Epoch 287/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5342 - accuracy: 0.5191 - val_loss: 1.7198 - val_accuracy: 0.4765\n",
      "Epoch 288/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5212 - accuracy: 0.5172 - val_loss: 1.7251 - val_accuracy: 0.4686\n",
      "Epoch 289/400\n",
      "1568/1568 [==============================] - 32s 21ms/step - loss: 1.5147 - accuracy: 0.5222 - val_loss: 1.7042 - val_accuracy: 0.4723\n",
      "Epoch 290/400\n",
      "1568/1568 [==============================] - 33s 21ms/step - loss: 1.5292 - accuracy: 0.5164 - val_loss: 1.7347 - val_accuracy: 0.4667\n",
      "Epoch 291/400\n",
      " 465/1568 [=======>......................] - ETA: 20s - loss: 1.5277 - accuracy: 0.5238"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-00f53680d987>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# alongside the top Dense layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m history = lenet.fit(\n\u001b[0m\u001b[0;32m      5\u001b[0m       \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# One pass through entire training dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.4-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "\n",
    "history = lenet.fit(\n",
    "      train_data,\n",
    "      steps_per_epoch= train_data.samples // BATCH_SIZE,  # One pass through entire training dataset\n",
    "      epochs=400,\n",
    "      validation_data=val_data,\n",
    "      validation_steps= val_data.samples // BATCH_SIZE,  # One pass through entire validation dataset\n",
    "      #validation_freq=10,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/lenet_500_epochs\\assets\n"
     ]
    }
   ],
   "source": [
    "lenet.save('trained_model/lenet_500_epochs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "TARGET_SIZE = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31368 images belonging to 43 classes.\n",
      "Found 7841 images belonging to 43 classes.\n",
      "Found 12630 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a data generator for the training images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)  # val 20%\n",
    "\n",
    "# Create a data generator for the validation images\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "#Split data to training and validation datasets\n",
    "train_data = train_datagen.flow_from_directory(train_path, \n",
    "                                               target_size=TARGET_SIZE, \n",
    "                                               color_mode='rgb',\n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               seed=2,\n",
    "                                               subset = 'training') \n",
    "\n",
    "val_data = val_datagen.flow_from_directory(train_path, \n",
    "                                           target_size=TARGET_SIZE, \n",
    "                                           color_mode='rgb',\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=False,\n",
    "                                           seed=2,\n",
    "                                           subset = 'validation')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_data = datagen.flow_from_directory(test_path,\n",
    "                                        target_size=TARGET_SIZE, \n",
    "                                        color_mode='rgb',\n",
    "                                        class_mode='categorical',\n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "156/156 [==============================] - 28s 179ms/step - loss: 1.4984 - accuracy: 0.5241 - val_loss: 1.6963 - val_accuracy: 0.4779\n",
      "Epoch 2/50\n",
      "156/156 [==============================] - 28s 179ms/step - loss: 1.4985 - accuracy: 0.5267 - val_loss: 1.6989 - val_accuracy: 0.4763\n",
      "Epoch 3/50\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 1.5042 - accuracy: 0.5278 - val_loss: 1.6896 - val_accuracy: 0.4783\n",
      "Epoch 4/50\n",
      "156/156 [==============================] - 28s 178ms/step - loss: 1.5037 - accuracy: 0.5262 - val_loss: 1.6932 - val_accuracy: 0.4827\n",
      "Epoch 5/50\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.5091 - accuracy: 0.5250 - val_loss: 1.6976 - val_accuracy: 0.4767\n",
      "Epoch 6/50\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 1.5083 - accuracy: 0.5220 - val_loss: 1.6925 - val_accuracy: 0.4791\n",
      "Epoch 7/50\n",
      "156/156 [==============================] - 30s 193ms/step - loss: 1.5047 - accuracy: 0.5293 - val_loss: 1.6971 - val_accuracy: 0.4781\n",
      "Epoch 8/50\n",
      "156/156 [==============================] - 29s 188ms/step - loss: 1.5019 - accuracy: 0.5313 - val_loss: 1.6932 - val_accuracy: 0.4768\n",
      "Epoch 9/50\n",
      "156/156 [==============================] - 29s 183ms/step - loss: 1.5078 - accuracy: 0.5258 - val_loss: 1.6936 - val_accuracy: 0.4800\n",
      "Epoch 10/50\n",
      "156/156 [==============================] - 30s 194ms/step - loss: 1.5077 - accuracy: 0.5245 - val_loss: 1.6979 - val_accuracy: 0.4794\n",
      "Epoch 11/50\n",
      "156/156 [==============================] - 30s 191ms/step - loss: 1.5028 - accuracy: 0.5272 - val_loss: 1.6990 - val_accuracy: 0.4801\n",
      "Epoch 12/50\n",
      "156/156 [==============================] - 31s 202ms/step - loss: 1.4986 - accuracy: 0.5248 - val_loss: 1.6900 - val_accuracy: 0.4805\n",
      "Epoch 13/50\n",
      "156/156 [==============================] - 31s 201ms/step - loss: 1.5114 - accuracy: 0.5249 - val_loss: 1.6945 - val_accuracy: 0.4771\n",
      "Epoch 14/50\n",
      "156/156 [==============================] - 31s 199ms/step - loss: 1.5048 - accuracy: 0.5227 - val_loss: 1.6949 - val_accuracy: 0.4818\n",
      "Epoch 15/50\n",
      "156/156 [==============================] - 29s 188ms/step - loss: 1.5021 - accuracy: 0.5272 - val_loss: 1.6942 - val_accuracy: 0.4799\n",
      "Epoch 16/50\n",
      "156/156 [==============================] - 29s 183ms/step - loss: 1.5040 - accuracy: 0.5266 - val_loss: 1.6908 - val_accuracy: 0.4826\n",
      "Epoch 17/50\n",
      "156/156 [==============================] - 31s 200ms/step - loss: 1.5023 - accuracy: 0.5252 - val_loss: 1.6941 - val_accuracy: 0.4824\n",
      "Epoch 18/50\n",
      "156/156 [==============================] - 30s 195ms/step - loss: 1.4926 - accuracy: 0.5260 - val_loss: 1.6935 - val_accuracy: 0.4808\n",
      "Epoch 19/50\n",
      "156/156 [==============================] - 32s 203ms/step - loss: 1.5002 - accuracy: 0.5292 - val_loss: 1.6956 - val_accuracy: 0.4790\n",
      "Epoch 20/50\n",
      "156/156 [==============================] - 31s 201ms/step - loss: 1.4946 - accuracy: 0.5308 - val_loss: 1.6908 - val_accuracy: 0.4805\n",
      "Epoch 21/50\n",
      "156/156 [==============================] - 31s 198ms/step - loss: 1.4998 - accuracy: 0.5280 - val_loss: 1.6929 - val_accuracy: 0.4790\n",
      "Epoch 22/50\n",
      "156/156 [==============================] - 31s 200ms/step - loss: 1.5007 - accuracy: 0.5261 - val_loss: 1.6925 - val_accuracy: 0.4794\n",
      "Epoch 23/50\n",
      "156/156 [==============================] - 29s 188ms/step - loss: 1.5044 - accuracy: 0.5276 - val_loss: 1.6890 - val_accuracy: 0.4804\n",
      "Epoch 24/50\n",
      "156/156 [==============================] - 29s 188ms/step - loss: 1.4976 - accuracy: 0.5244 - val_loss: 1.6833 - val_accuracy: 0.4837\n",
      "Epoch 25/50\n",
      "156/156 [==============================] - 29s 189ms/step - loss: 1.5076 - accuracy: 0.5255 - val_loss: 1.6947 - val_accuracy: 0.4809\n",
      "Epoch 26/50\n",
      "156/156 [==============================] - 29s 187ms/step - loss: 1.4934 - accuracy: 0.5288 - val_loss: 1.6955 - val_accuracy: 0.4796\n",
      "Epoch 27/50\n",
      "156/156 [==============================] - 31s 201ms/step - loss: 1.4981 - accuracy: 0.5281 - val_loss: 1.6888 - val_accuracy: 0.4826\n",
      "Epoch 28/50\n",
      "156/156 [==============================] - 29s 187ms/step - loss: 1.5067 - accuracy: 0.5259 - val_loss: 1.6902 - val_accuracy: 0.4817\n",
      "Epoch 29/50\n",
      "156/156 [==============================] - 29s 186ms/step - loss: 1.5022 - accuracy: 0.5272 - val_loss: 1.6940 - val_accuracy: 0.4779\n",
      "Epoch 30/50\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 1.5036 - accuracy: 0.5257 - val_loss: 1.7041 - val_accuracy: 0.4753\n",
      "Epoch 31/50\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 1.4991 - accuracy: 0.5305 - val_loss: 1.6968 - val_accuracy: 0.4791\n",
      "Epoch 32/50\n",
      "156/156 [==============================] - 29s 186ms/step - loss: 1.5061 - accuracy: 0.5232 - val_loss: 1.6867 - val_accuracy: 0.4783\n",
      "Epoch 33/50\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.5064 - accuracy: 0.5246 - val_loss: 1.6919 - val_accuracy: 0.4790\n",
      "Epoch 34/50\n",
      "156/156 [==============================] - 29s 188ms/step - loss: 1.4923 - accuracy: 0.5302 - val_loss: 1.6973 - val_accuracy: 0.4779\n",
      "Epoch 35/50\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.4991 - accuracy: 0.5292 - val_loss: 1.6934 - val_accuracy: 0.4796\n",
      "Epoch 36/50\n",
      "156/156 [==============================] - 29s 186ms/step - loss: 1.4989 - accuracy: 0.5293 - val_loss: 1.6963 - val_accuracy: 0.4805\n",
      "Epoch 37/50\n",
      "156/156 [==============================] - 30s 194ms/step - loss: 1.4997 - accuracy: 0.5283 - val_loss: 1.7018 - val_accuracy: 0.4772\n",
      "Epoch 38/50\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 1.4998 - accuracy: 0.5263 - val_loss: 1.6928 - val_accuracy: 0.4821\n",
      "Epoch 39/50\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.5125 - accuracy: 0.5268 - val_loss: 1.6973 - val_accuracy: 0.4787\n",
      "Epoch 40/50\n",
      "156/156 [==============================] - 28s 183ms/step - loss: 1.4987 - accuracy: 0.5269 - val_loss: 1.6916 - val_accuracy: 0.4823\n",
      "Epoch 41/50\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 1.4967 - accuracy: 0.5295 - val_loss: 1.7033 - val_accuracy: 0.4773\n",
      "Epoch 42/50\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.4880 - accuracy: 0.5310 - val_loss: 1.6918 - val_accuracy: 0.4800\n",
      "Epoch 43/50\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 1.4983 - accuracy: 0.5277 - val_loss: 1.6908 - val_accuracy: 0.4832\n",
      "Epoch 44/50\n",
      "156/156 [==============================] - 28s 182ms/step - loss: 1.5116 - accuracy: 0.5272 - val_loss: 1.6898 - val_accuracy: 0.4814\n",
      "Epoch 45/50\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 1.5070 - accuracy: 0.5240 - val_loss: 1.6893 - val_accuracy: 0.4809\n",
      "Epoch 46/50\n",
      "156/156 [==============================] - 28s 181ms/step - loss: 1.4880 - accuracy: 0.5335 - val_loss: 1.6808 - val_accuracy: 0.4821\n",
      "Epoch 47/50\n",
      "156/156 [==============================] - 28s 180ms/step - loss: 1.4913 - accuracy: 0.5290 - val_loss: 1.6897 - val_accuracy: 0.4787\n",
      "Epoch 48/50\n",
      "156/156 [==============================] - 29s 184ms/step - loss: 1.4989 - accuracy: 0.5271 - val_loss: 1.7005 - val_accuracy: 0.4777\n",
      "Epoch 49/50\n",
      "156/156 [==============================] - 28s 179ms/step - loss: 1.5019 - accuracy: 0.5274 - val_loss: 1.6913 - val_accuracy: 0.4768\n",
      "Epoch 50/50\n",
      "156/156 [==============================] - 29s 187ms/step - loss: 1.4952 - accuracy: 0.5272 - val_loss: 1.6944 - val_accuracy: 0.4809\n"
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "\n",
    "history = lenet.fit(\n",
    "      train_data,\n",
    "      steps_per_epoch= train_data.samples // BATCH_SIZE,  # One pass through entire training dataset\n",
    "      epochs=50,\n",
    "      validation_data=val_data,\n",
    "      validation_steps= val_data.samples // BATCH_SIZE,  # One pass through entire validation dataset\n",
    "      #validation_freq=10,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
